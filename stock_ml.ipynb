{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJDzbgv2Zf4yBf3WGJW3yb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csieung/ml/blob/main/stock_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vds-7e_qPLH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb69ef3-64d5-490f-9d15-a3eecbff11a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.39.9)\n",
            "Collecting botocore<1.40.0,>=1.39.9 (from boto3)\n",
            "  Using cached botocore-1.39.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.9->boto3) (1.17.0)\n",
            "Using cached botocore-1.39.9-py3-none-any.whl (13.9 MB)\n",
            "Installing collected packages: botocore\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.38.46\n",
            "    Uninstalling botocore-1.38.46:\n",
            "      Successfully uninstalled botocore-1.38.46\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiobotocore 2.23.1 requires botocore<1.38.47,>=1.38.40, but you have botocore 1.39.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed botocore-1.39.9\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Ï†ÑÏ≤òÎ¶¨ -> s3 Ï†ÄÏû•(feature table)"
      ],
      "metadata": {
        "id": "d_HFVn0EsGi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import boto3\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def load_data(target_date: str) -> pd.DataFrame:\n",
        "    aws_access_key_id = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    aws_secret_access_key = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "        region_name = 'us-east-1'\n",
        "    )\n",
        "\n",
        "    bucket_name = 'de6-team7-bucket'\n",
        "    prefix = f\"raw_stock/stock_dt={target_date}/\"\n",
        "    print(f\"S3 Í≤ΩÎ°úÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨ÏòµÎãàÎã§: s3://{bucket_name}/{prefix}\")\n",
        "\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "        if 'Contents' not in response:\n",
        "            print(f\"Ïò§Î•ò: Í≤ΩÎ°ú '{prefix}'Ïóê ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
        "            return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"S3 ÌååÏùº Î™©Î°ùÏùÑ Í∞ÄÏ†∏Ïò§Îäî Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    file_keys = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
        "    if not file_keys:\n",
        "        print(f\"Í≤ΩÎ°ú '{prefix}'ÏóêÏÑú Parquet ÌååÏùºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.\")\n",
        "        return pd.DataFrame()\n",
        "    print(f\"Ï¥ù {len(file_keys)}Í∞úÏùò Parquet ÌååÏùºÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§.\")\n",
        "\n",
        "    df_list = []\n",
        "    for key in file_keys:\n",
        "        s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
        "        buffer = io.BytesIO(s3_object['Body'].read())\n",
        "        df_temp = pd.read_parquet(buffer)\n",
        "        df_list.append(df_temp)\n",
        "\n",
        "    if not df_list:\n",
        "        print('Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨Ïò§ÏßÄ Î™ªÌñàÏäµÎãàÎã§.')\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    raw_combined = pd.concat(df_list, ignore_index=True)\n",
        "    print(f\"Ï¥ù {len(raw_combined):,}Í∞úÏùò Î°úÏö∞(Îç∞Ïù¥ÌÑ∞)Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôîÏäµÎãàÎã§.\")\n",
        "    return raw_combined\n",
        "\n",
        "def feature_table(raw_df: pd.DataFrame, target_date: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ï£ºÏãù ÏõêÏãú Îç∞Ïù¥ÌÑ∞Î•º 1Î∂Ñ Îã®ÏúÑÎ°ú ÏßëÍ≥ÑÌï©ÎãàÎã§. (ÌõÑÏ≤òÎ¶¨ Î°úÏßÅÏùÑ Îã®ÏàúÌôîÌïòÏó¨ ÏïàÏ†ïÏÑ±ÏùÑ ÌôïÎ≥¥Ìïú ÏµúÏ¢Ö Î≤ÑÏ†Ñ)\n",
        "    \"\"\"\n",
        "    df = raw_df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='s', utc=True) \\\n",
        "                      .dt.tz_convert('Asia/Seoul') \\\n",
        "                      .dt.tz_localize(None)\n",
        "    df = df.sort_values('timestamp')\n",
        "\n",
        "    required_cols = ['timestamp_ms', 'stock_code', 'stock_name', 'trade_price', 'antc_volume', 'antc_amount',\n",
        "                     'ask_price_1', 'bid_price_1', 'bid_volume_1', 'ask_volume_1',\n",
        "                     'total_bid_volume', 'total_ask_volume']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        print(f\"üö® Ïò§Î•ò: ÌïÑÏàò Ïª¨ÎüºÏù¥ ÎàÑÎùΩÎêòÏóàÏäµÎãàÎã§ - {missing}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df['spread'] = df['ask_price_1'] - df['bid_price_1']\n",
        "    df['orderbook_imbalance'] = df['bid_volume_1'] / (df['bid_volume_1'] + df['ask_volume_1']).replace(0, np.nan)\n",
        "    df['total_book_imbalance'] = df['total_bid_volume'] / (df['total_bid_volume'] + df['total_ask_volume']).replace(0, np.nan)\n",
        "\n",
        "    processed_groups = []\n",
        "\n",
        "    # stock_code Î≥ÑÎ°ú Í∑∏Î£πÏùÑ ÎÇòÎàÑÏñ¥ Î∞òÎ≥µ Ï≤òÎ¶¨\n",
        "    for code, group_df in df.groupby('stock_code'):\n",
        "        resampler = group_df.set_index('timestamp').resample('1h')\n",
        "\n",
        "        ohlc_df = resampler['trade_price'].ohlc()\n",
        "        volume_df = resampler['antc_volume'].sum().to_frame('volume_1h')\n",
        "        amount_df = resampler['antc_amount'].sum().to_frame('amount_1h')\n",
        "        spread_df = resampler['spread'].mean().to_frame('spread_mean_1h')\n",
        "        imbalance_df = resampler['orderbook_imbalance'].mean().to_frame('orderbook_imbalance_1h')\n",
        "        total_imbalance_df = resampler['total_book_imbalance'].mean().to_frame('total_book_imbalance_1h')\n",
        "\n",
        "        minute_df = pd.concat([\n",
        "            ohlc_df, volume_df, amount_df, spread_df, imbalance_df, total_imbalance_df\n",
        "        ], axis=1)\n",
        "\n",
        "        minute_df.rename(columns={\n",
        "            'open': 'open_price_1h', 'high': 'high_price_1h',\n",
        "            'low': 'low_price_1h', 'close': 'close_price_1h'\n",
        "        }, inplace=True)\n",
        "\n",
        "        minute_df['stock_name'] = group_df['stock_name'].iloc[0] if not group_df.empty else ''\n",
        "        minute_df['stock_code'] = code\n",
        "\n",
        "        processed_groups.append(minute_df)\n",
        "\n",
        "    if not processed_groups:\n",
        "        print(\"Ï≤òÎ¶¨Ìï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_df = pd.concat(processed_groups).reset_index(drop=False)\n",
        "\n",
        "    cols_to_ffill = final_df.columns.difference(['timestamp', 'stock_code', 'stock_name'])\n",
        "    final_df[cols_to_ffill] = final_df.groupby('stock_code')[cols_to_ffill].ffill()\n",
        "\n",
        "    final_df['vwap_1h'] = final_df['amount_1h'] / final_df['volume_1h'].replace(0, np.nan)\n",
        "\n",
        "    grp = final_df.groupby('stock_code')\n",
        "\n",
        "    final_df['ma_5h'] = grp['close_price_1h'].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    final_df['ma_20h'] = grp['close_price_1h'].rolling(20, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    rolling_std_20 = grp['close_price_1h'].rolling(20, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "    #final_df['boll_upper'] = final_df['ma_20m'] + 2 * rolling_std_20\n",
        "    #final_df['boll_lower'] = final_df['ma_20m'] - 2 * rolling_std_20\n",
        "\n",
        "\n",
        "    final_df.dropna(subset=['close_price_1h'], inplace=True)\n",
        "\n",
        "    final_cols = [\n",
        "        'timestamp', 'stock_code', 'stock_name', 'open_price_1h', 'high_price_1h',\n",
        "        'low_price_1h', 'close_price_1h', 'volume_1h', 'vwap_1h', 'spread_mean_1h',\n",
        "        'orderbook_imbalance_1h', 'total_book_imbalance_1h', 'ma_5h', 'ma_20h'\n",
        "    ]\n",
        "\n",
        "    existing_cols = [col for col in final_cols if col in final_df.columns]\n",
        "    final_df = final_df[existing_cols].copy()\n",
        "\n",
        "    final_df = final_df.sort_values(['stock_code', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def save_to_s3(df: pd.DataFrame, target_date:str):\n",
        "    if df.empty:\n",
        "        print(\"Ï†ÄÏû•Îêú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    bucket_name = 'de6-team7-bucket'\n",
        "    output_path = f\"s3://{bucket_name}/processed_stock/stock_dt={target_date}/features.parquet\"\n",
        "\n",
        "    print(f\"\\nÏ†ÑÏ≤òÎ¶¨Îêú ÌîºÏ≤ò ÌÖåÏù¥Î∏îÏùÑ S3Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§...\")\n",
        "    print(f\"Í≤ΩÎ°ú: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        storage_options = {\n",
        "            'key': userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "            'secret': userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "        }\n",
        "        df.to_parquet(\n",
        "            output_path,\n",
        "            index=False,\n",
        "            engine='pyarrow',\n",
        "            storage_options=storage_options\n",
        "        )\n",
        "        print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å!\")\n",
        "    except Exception as e:\n",
        "        print(f\"üö® S3 Ï†ÄÏû• Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_date = '2025-07-11'\n",
        "    print(f\"--- Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏãúÏûë ({target_date}) ---\")\n",
        "\n",
        "    raw_data = load_data(target_date)\n",
        "\n",
        "    if not raw_data.empty:\n",
        "        feature_df = feature_table(raw_data, target_date)\n",
        "        print(feature_df.head())\n",
        "        save_to_s3(feature_df, target_date)\n",
        "    else:\n",
        "        print(f\"{target_date}Ïóê Ï≤òÎ¶¨Ìï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "\n",
        "    print(f\"--- Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏôÑÎ£å ({target_date}) ---\")\n"
      ],
      "metadata": {
        "id": "nR28Q9RPeKqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11ecbf3-e33a-4605-a02f-6388a96cf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏãúÏûë (2025-07-11) ---\n",
            "S3 Í≤ΩÎ°úÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Î∂àÎü¨ÏòµÎãàÎã§: s3://de6-team7-bucket/raw_stock/stock_dt=2025-07-11/\n",
            "Ï¥ù 20Í∞úÏùò Parquet ÌååÏùºÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§.\n",
            "Ï¥ù 13,327Í∞úÏùò Î°úÏö∞(Îç∞Ïù¥ÌÑ∞)Î•º ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î∂àÎü¨ÏôîÏäµÎãàÎã§.\n",
            "            timestamp stock_code stock_name  open_price_1h  high_price_1h  \\\n",
            "0 2025-07-11 10:00:00     000020       ÎèôÌôîÏïΩÌíà         6950.0         6950.0   \n",
            "1 2025-07-11 11:00:00     000020       ÎèôÌôîÏïΩÌíà         6910.0         6910.0   \n",
            "2 2025-07-11 12:00:00     000020       ÎèôÌôîÏïΩÌíà         6930.0         6940.0   \n",
            "3 2025-07-11 13:00:00     000020       ÎèôÌôîÏïΩÌíà         6910.0         6910.0   \n",
            "4 2025-07-11 14:00:00     000020       ÎèôÌôîÏïΩÌíà         6910.0         6910.0   \n",
            "\n",
            "   low_price_1h  close_price_1h  volume_1h  vwap_1h  spread_mean_1h  \\\n",
            "0        6940.0          6940.0       1836   6890.0            10.0   \n",
            "1        6910.0          6910.0        612   6890.0            20.0   \n",
            "2        6930.0          6940.0       1224   6890.0            15.0   \n",
            "3        6910.0          6910.0        612   6890.0            20.0   \n",
            "4        6910.0          6910.0        612   6890.0            10.0   \n",
            "\n",
            "   orderbook_imbalance_1h  total_book_imbalance_1h   ma_5h  ma_20h  \n",
            "0                0.538861                 0.827538  6940.0  6940.0  \n",
            "1                0.930549                 0.853946  6925.0  6925.0  \n",
            "2                0.438783                 0.868749  6930.0  6930.0  \n",
            "3                0.990295                 0.817703  6925.0  6925.0  \n",
            "4                0.930493                 0.647179  6922.0  6922.0  \n",
            "\n",
            "Ï†ÑÏ≤òÎ¶¨Îêú ÌîºÏ≤ò ÌÖåÏù¥Î∏îÏùÑ S3Ïóê Ï†ÄÏû•Ìï©ÎãàÎã§...\n",
            "Í≤ΩÎ°ú: s3://de6-team7-bucket/processed_stock/stock_dt=2025-07-11/features.parquet\n",
            "‚úÖ Ï†ÄÏû• ÏôÑÎ£å!\n",
            "--- Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨ ÏôÑÎ£å (2025-07-11) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install s3fs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSxFV_-w7Ft",
        "outputId": "12289ee2-4f31-47e5-c6d2-9a1438506706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.11/dist-packages (2025.7.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from s3fs) (2.23.1)\n",
            "Requirement already satisfied: fsspec==2025.7.0 in /usr/local/lib/python3.11/dist-packages (from s3fs) (2025.7.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from s3fs) (3.11.15)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.38.47,>=1.38.40 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.38.46)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.6.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.14.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.47,>=1.38.40->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-connector-python[pandas]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbcEuHwkWsOs",
        "outputId": "e3e3f139-424b-46f6-e1c7-ec6111f0b665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python[pandas]\n",
            "  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: boto3>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.39.9)\n",
            "Requirement already satisfied: botocore>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.39.9)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (0.13.3)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (18.1.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python[pandas]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python[pandas]) (0.13.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.22)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python[pandas]) (1.17.0)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 1ÏãúÍ∞Ñ Îí§ Í∞ÄÍ≤© ÏòàÏ∏°(ÌöåÍ∑Ä)\n"
      ],
      "metadata": {
        "id": "z8BdkMfbsM-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import snowflake.connector\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "def get_snowflake_connection():\n",
        "    return snowflake.connector.connect(\n",
        "        user=userdata.get('SNOWFLAKE_USER'),\n",
        "        password=userdata.get('SNOWFLAKE_PASSWORD'),\n",
        "        account=userdata.get('SNOWFLAKE_ACCOUNT'),\n",
        "        warehouse=userdata.get('SNOWFLAKE_WAREHOUSE'),\n",
        "        database=userdata.get('SNOWFLAKE_DATABASE'),\n",
        "        schema=userdata.get('SNOWFLAKE_SCHEMA')\n",
        "    )\n",
        "\n",
        "# --- ÎßàÏºìÎ≥Ñ Í∞ÄÍ≤© ÏòàÏ∏° Î™®Îç∏ ÌïôÏäµ (ÌöåÍ∑Ä) ---\n",
        "def train_stock_regression_model(market_df):\n",
        "    df = market_df.copy()\n",
        "\n",
        "    # Target Î≥ÄÏàò ÏÉùÏÑ± ('1ÏãúÍ∞Ñ Îí§ Ï¢ÖÍ∞Ä')\n",
        "    df['future_1h_price'] = df['close_price_1h'].shift(-1)\n",
        "    df.dropna(subset=['future_1h_price'], inplace=True)\n",
        "    if df.empty:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    cols_to_drop = df.columns[df.isna().all()].tolist()\n",
        "    feature_cols = df.columns.difference([\n",
        "        'stock_code', 'stock_name', 'timestamp', 'future_1h_price'\n",
        "    ] + cols_to_drop)\n",
        "\n",
        "    X, y = df[feature_cols], df['future_1h_price']\n",
        "\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    X_train, X_test = X.iloc[:split_idx].copy(), X.iloc[split_idx:].copy()\n",
        "    y_train, y_test = y.iloc[:split_idx].copy(), y.iloc[split_idx:].copy()\n",
        "\n",
        "    model = lgb.LGBMRegressor(objective='regression_l1', random_state=42)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    result_df = pd.DataFrame({\n",
        "        'timestamp': df.loc[y_test.index, 'timestamp'],\n",
        "        'stock_code': df.loc[y_test.index, 'stock_code'],\n",
        "        'stock_name': df.loc[y_test.index, 'stock_name'],\n",
        "        'actual_price': y_test,\n",
        "        'predicted_price': predictions\n",
        "    })\n",
        "\n",
        "    return mae, result_df\n",
        "\n",
        "def train_and_predict_regression(df):\n",
        "    \"\"\"Î≥ëÎ†¨ Ï≤òÎ¶¨Î•º ÌÜµÌï¥ Î™®Îì† Ï¢ÖÎ™©Ïùò ÌöåÍ∑Ä Î™®Îç∏ÏùÑ ÌïôÏäµÌïòÍ≥† ÏòàÏ∏° Í≤∞Í≥ºÎ•º Ï¢ÖÌï©Ìï©ÎãàÎã§.\"\"\"\n",
        "    print(\"\\n--- ML Í∞ÄÍ≤© ÏòàÏ∏°(ÌöåÍ∑Ä) Î™®Îç∏ ÌïôÏäµ ÏãúÏûë ---\")\n",
        "    if df.empty:\n",
        "        print(\"ÌïôÏäµÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_stocks = [group.copy() for _, group in df.groupby('stock_code')]\n",
        "    results = []\n",
        "    market_maes = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = {executor.submit(train_stock_regression_model, stock_df): stock_df['stock_code'].iloc[0] for stock_df in all_stocks}\n",
        "        for future in futures:\n",
        "            stock_code = futures[future]\n",
        "            try:\n",
        "                mae, res_df = future.result()\n",
        "                if res_df is not None:\n",
        "                    results.append(res_df)\n",
        "                    market_maes[stock_code] = mae\n",
        "            except Exception as exc:\n",
        "                print(f'{stock_code} Î™®Îç∏ ÌïôÏäµ Ï§ë Ïò§Î•ò Î∞úÏÉù: {exc}')\n",
        "\n",
        "    if market_maes:\n",
        "        print(\"\\n--- ÏÉÅÏúÑ 5Í∞ú Î™®Îç∏ ÏÑ±Îä• (MAEÍ∞Ä ÎÇÆÏùÄ Ïàú) ---\")\n",
        "        sorted_maes = sorted(market_maes.items(), key=lambda item: item[1])\n",
        "        for code, mae in sorted_maes[:5]:\n",
        "            print(f\"Ï¢ÖÎ™© ÏΩîÎìú: {code}, MAE: {mae:.4f}\")\n",
        "        print(\"------------------------------------\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_df = pd.concat(results, ignore_index=True)\n",
        "    print(\"‚úÖ ÌïôÏäµ Î∞è ÏòàÏ∏° ÏôÑÎ£å\")\n",
        "    return final_df\n",
        "\n",
        "def upload_to_snowflake(df, table_name, target_date):\n",
        "    if df.empty:\n",
        "        print(\"ÏóÖÎ°úÎìúÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
        "        return\n",
        "\n",
        "    staging_table_name = f\"{table_name}_STAGING\"\n",
        "    df['processed_dt'] = pd.to_datetime(target_date).strftime('%Y-%m-%d')\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    conn = None\n",
        "    try:\n",
        "        print(f\"\\n--- SnowflakeÏóê Í≤∞Í≥º Ï†ÅÏû¨ ÏãúÏûë ---\")\n",
        "        conn = get_snowflake_connection()\n",
        "        if conn:\n",
        "            write_pandas(conn, df, staging_table_name.upper(), auto_create_table=True, overwrite=True)\n",
        "            print(f\"‚úÖ Staging ÌÖåÏù¥Î∏î({staging_table_name}) Ï†ÅÏû¨ ÏÑ±Í≥µ: {df.shape[0]} Ìñâ\")\n",
        "\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            delete_query = f\"DELETE FROM {table_name.upper()} WHERE PROCESSED_DT = '{target_date}'\"\n",
        "            cursor.execute(delete_query)\n",
        "            print(f\"‚úÖ Î≥∏ ÌÖåÏù¥Î∏î({table_name})ÏóêÏÑú Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú ÏôÑÎ£å: {cursor.rowcount} Ìñâ\")\n",
        "\n",
        "            insert_query = f\"INSERT INTO {table_name.upper()} SELECT * FROM {staging_table_name.upper()}\"\n",
        "            cursor.execute(insert_query)\n",
        "            print(\"‚úÖ Î≥∏ ÌÖåÏù¥Î∏îÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô ÏôÑÎ£å!\")\n",
        "\n",
        "            cursor.close()\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Snowflake Ï†ÅÏû¨ Ïò§Î•ò: {e}\")\n",
        "    finally:\n",
        "        if conn is not None:\n",
        "            conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if 'feature_df' in locals() and not feature_df.empty:\n",
        "        predictions_df = train_and_predict_regression(feature_df)\n",
        "\n",
        "        if not predictions_df.empty:\n",
        "            print(\"\\n--- ÏµúÏ¢Ö ÏòàÏ∏° Í≤∞Í≥º (ÏÉÅÏúÑ 5Í∞ú) ---\")\n",
        "            print(predictions_df.head())\n",
        "            print(\"--------------------------------\")\n",
        "            upload_to_snowflake(predictions_df, 'STOCK_PRICE_PREDICTION', target_date)\n",
        "    else:\n",
        "        print(\"Ïò§Î•ò: 'feature_df'Í∞Ä Ï§ÄÎπÑÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Ïù¥Ï†Ñ ÏÖÄÏùÑ Î®ºÏ†Ä Ïã§ÌñâÌï¥Ï£ºÏÑ∏Ïöî.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCiqBbPXhI8-",
        "outputId": "4b96f1e3-cc76-4812-8fd4-47436aa2753d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ML Í∞ÄÍ≤© ÏòàÏ∏°(ÌöåÍ∑Ä) Î™®Îç∏ ÌïôÏäµ ÏãúÏûë ---\n",
            "\n",
            "--- ÏÉÅÏúÑ 5Í∞ú Î™®Îç∏ ÏÑ±Îä• (MAEÍ∞Ä ÎÇÆÏùÄ Ïàú) ---\n",
            "Ï¢ÖÎ™© ÏΩîÎìú: 000020, MAE: 0.0000\n",
            "Ï¢ÖÎ™© ÏΩîÎìú: 000300, MAE: 0.0000\n",
            "Ï¢ÖÎ™© ÏΩîÎìú: 000430, MAE: 0.0000\n",
            "Ï¢ÖÎ™© ÏΩîÎìú: 000520, MAE: 0.0000\n",
            "Ï¢ÖÎ™© ÏΩîÎìú: 000650, MAE: 0.0000\n",
            "------------------------------------\n",
            "‚úÖ ÌïôÏäµ Î∞è ÏòàÏ∏° ÏôÑÎ£å\n",
            "\n",
            "--- ÏµúÏ¢Ö ÏòàÏ∏° Í≤∞Í≥º (ÏÉÅÏúÑ 5Í∞ú) ---\n",
            "            timestamp stock_code stock_name  actual_price  predicted_price\n",
            "0 2025-07-11 13:00:00     000020       ÎèôÌôîÏïΩÌíà        6910.0           6910.0\n",
            "1 2025-07-11 13:00:00     000040      KRÎ™®ÌÑ∞Ïä§         430.0            431.0\n",
            "2 2025-07-11 13:00:00     000050         Í≤ΩÎ∞©        8150.0           8190.0\n",
            "3 2025-07-11 13:00:00     000070      ÏÇºÏñëÌôÄÎî©Ïä§      100100.0          99500.0\n",
            "4 2025-07-11 13:00:00     000080      ÌïòÏù¥Ìä∏ÏßÑÎ°ú       21600.0          21675.0\n",
            "--------------------------------\n",
            "\n",
            "--- SnowflakeÏóê Í≤∞Í≥º Ï†ÅÏû¨ ÏãúÏûë ---\n",
            "‚úÖ Staging ÌÖåÏù¥Î∏î(STOCK_PRICE_PREDICTION_STAGING) Ï†ÅÏû¨ ÏÑ±Í≥µ: 2770 Ìñâ\n",
            "‚úÖ Î≥∏ ÌÖåÏù¥Î∏î(STOCK_PRICE_PREDICTION)ÏóêÏÑú Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú ÏôÑÎ£å: 25693 Ìñâ\n",
            "‚úÖ Î≥∏ ÌÖåÏù¥Î∏îÎ°ú Îç∞Ïù¥ÌÑ∞ Ïù¥Îèô ÏôÑÎ£å!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"forkin\"+\"me\")"
      ],
      "metadata": {
        "id": "btivchmWos5j",
        "outputId": "6a3fdc86-bf3c-4a6e-f82f-8aec21982b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forkinme\n"
          ]
        }
      ]
    }
  ]
}