{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJDzbgv2Zf4yBf3WGJW3yb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csieung/ml/blob/main/stock_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vds-7e_qPLH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb69ef3-64d5-490f-9d15-a3eecbff11a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (1.39.9)\n",
            "Collecting botocore<1.40.0,>=1.39.9 (from boto3)\n",
            "  Using cached botocore-1.39.9-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3) (0.13.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.9->boto3) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.9->boto3) (1.17.0)\n",
            "Using cached botocore-1.39.9-py3-none-any.whl (13.9 MB)\n",
            "Installing collected packages: botocore\n",
            "  Attempting uninstall: botocore\n",
            "    Found existing installation: botocore 1.38.46\n",
            "    Uninstalling botocore-1.38.46:\n",
            "      Successfully uninstalled botocore-1.38.46\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "aiobotocore 2.23.1 requires botocore<1.38.47,>=1.38.40, but you have botocore 1.39.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed botocore-1.39.9\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. ì „ì²˜ë¦¬ -> s3 ì €ì¥(feature table)"
      ],
      "metadata": {
        "id": "d_HFVn0EsGi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import boto3\n",
        "from datetime import datetime, timedelta\n",
        "import io\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "def load_data(target_date: str) -> pd.DataFrame:\n",
        "    aws_access_key_id = userdata.get('AWS_ACCESS_KEY_ID')\n",
        "    aws_secret_access_key = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "\n",
        "    s3_client = boto3.client(\n",
        "        's3',\n",
        "        aws_access_key_id=aws_access_key_id,\n",
        "        aws_secret_access_key=aws_secret_access_key,\n",
        "        region_name = 'us-east-1'\n",
        "    )\n",
        "\n",
        "    bucket_name = 'de6-team7-bucket'\n",
        "    prefix = f\"raw_stock/stock_dt={target_date}/\"\n",
        "    print(f\"S3 ê²½ë¡œì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: s3://{bucket_name}/{prefix}\")\n",
        "\n",
        "    try:\n",
        "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "        if 'Contents' not in response:\n",
        "            print(f\"ì˜¤ë¥˜: ê²½ë¡œ '{prefix}'ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "            return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"S3 íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "        return pd.DataFrame()\n",
        "    file_keys = [obj['Key'] for obj in response['Contents'] if obj['Key'].endswith('.parquet')]\n",
        "    if not file_keys:\n",
        "        print(f\"ê²½ë¡œ '{prefix}'ì—ì„œ Parquet íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame()\n",
        "    print(f\"ì´ {len(file_keys)}ê°œì˜ Parquet íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    df_list = []\n",
        "    for key in file_keys:\n",
        "        s3_object = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
        "        buffer = io.BytesIO(s3_object['Body'].read())\n",
        "        df_temp = pd.read_parquet(buffer)\n",
        "        df_list.append(df_temp)\n",
        "\n",
        "    if not df_list:\n",
        "        print('ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.')\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    raw_combined = pd.concat(df_list, ignore_index=True)\n",
        "    print(f\"ì´ {len(raw_combined):,}ê°œì˜ ë¡œìš°(ë°ì´í„°)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\")\n",
        "    return raw_combined\n",
        "\n",
        "def feature_table(raw_df: pd.DataFrame, target_date: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ì£¼ì‹ ì›ì‹œ ë°ì´í„°ë¥¼ 1ë¶„ ë‹¨ìœ„ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤. (í›„ì²˜ë¦¬ ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì•ˆì •ì„±ì„ í™•ë³´í•œ ìµœì¢… ë²„ì „)\n",
        "    \"\"\"\n",
        "    df = raw_df.copy()\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp_ms'], unit='s', utc=True) \\\n",
        "                      .dt.tz_convert('Asia/Seoul') \\\n",
        "                      .dt.tz_localize(None)\n",
        "    df = df.sort_values('timestamp')\n",
        "\n",
        "    required_cols = ['timestamp_ms', 'stock_code', 'stock_name', 'trade_price', 'antc_volume', 'antc_amount',\n",
        "                     'ask_price_1', 'bid_price_1', 'bid_volume_1', 'ask_volume_1',\n",
        "                     'total_bid_volume', 'total_ask_volume']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        print(f\"ğŸš¨ ì˜¤ë¥˜: í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤ - {missing}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df['spread'] = df['ask_price_1'] - df['bid_price_1']\n",
        "    df['orderbook_imbalance'] = df['bid_volume_1'] / (df['bid_volume_1'] + df['ask_volume_1']).replace(0, np.nan)\n",
        "    df['total_book_imbalance'] = df['total_bid_volume'] / (df['total_bid_volume'] + df['total_ask_volume']).replace(0, np.nan)\n",
        "\n",
        "    processed_groups = []\n",
        "\n",
        "    # stock_code ë³„ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ„ì–´ ë°˜ë³µ ì²˜ë¦¬\n",
        "    for code, group_df in df.groupby('stock_code'):\n",
        "        resampler = group_df.set_index('timestamp').resample('1h')\n",
        "\n",
        "        ohlc_df = resampler['trade_price'].ohlc()\n",
        "        volume_df = resampler['antc_volume'].sum().to_frame('volume_1h')\n",
        "        amount_df = resampler['antc_amount'].sum().to_frame('amount_1h')\n",
        "        spread_df = resampler['spread'].mean().to_frame('spread_mean_1h')\n",
        "        imbalance_df = resampler['orderbook_imbalance'].mean().to_frame('orderbook_imbalance_1h')\n",
        "        total_imbalance_df = resampler['total_book_imbalance'].mean().to_frame('total_book_imbalance_1h')\n",
        "\n",
        "        minute_df = pd.concat([\n",
        "            ohlc_df, volume_df, amount_df, spread_df, imbalance_df, total_imbalance_df\n",
        "        ], axis=1)\n",
        "\n",
        "        minute_df.rename(columns={\n",
        "            'open': 'open_price_1h', 'high': 'high_price_1h',\n",
        "            'low': 'low_price_1h', 'close': 'close_price_1h'\n",
        "        }, inplace=True)\n",
        "\n",
        "        minute_df['stock_name'] = group_df['stock_name'].iloc[0] if not group_df.empty else ''\n",
        "        minute_df['stock_code'] = code\n",
        "\n",
        "        processed_groups.append(minute_df)\n",
        "\n",
        "    if not processed_groups:\n",
        "        print(\"ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_df = pd.concat(processed_groups).reset_index(drop=False)\n",
        "\n",
        "    cols_to_ffill = final_df.columns.difference(['timestamp', 'stock_code', 'stock_name'])\n",
        "    final_df[cols_to_ffill] = final_df.groupby('stock_code')[cols_to_ffill].ffill()\n",
        "\n",
        "    final_df['vwap_1h'] = final_df['amount_1h'] / final_df['volume_1h'].replace(0, np.nan)\n",
        "\n",
        "    grp = final_df.groupby('stock_code')\n",
        "\n",
        "    final_df['ma_5h'] = grp['close_price_1h'].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    final_df['ma_20h'] = grp['close_price_1h'].rolling(20, min_periods=1).mean().reset_index(level=0, drop=True)\n",
        "    rolling_std_20 = grp['close_price_1h'].rolling(20, min_periods=1).std().reset_index(level=0, drop=True)\n",
        "    #final_df['boll_upper'] = final_df['ma_20m'] + 2 * rolling_std_20\n",
        "    #final_df['boll_lower'] = final_df['ma_20m'] - 2 * rolling_std_20\n",
        "\n",
        "\n",
        "    final_df.dropna(subset=['close_price_1h'], inplace=True)\n",
        "\n",
        "    final_cols = [\n",
        "        'timestamp', 'stock_code', 'stock_name', 'open_price_1h', 'high_price_1h',\n",
        "        'low_price_1h', 'close_price_1h', 'volume_1h', 'vwap_1h', 'spread_mean_1h',\n",
        "        'orderbook_imbalance_1h', 'total_book_imbalance_1h', 'ma_5h', 'ma_20h'\n",
        "    ]\n",
        "\n",
        "    existing_cols = [col for col in final_cols if col in final_df.columns]\n",
        "    final_df = final_df[existing_cols].copy()\n",
        "\n",
        "    final_df = final_df.sort_values(['stock_code', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "    return final_df\n",
        "\n",
        "def save_to_s3(df: pd.DataFrame, target_date:str):\n",
        "    if df.empty:\n",
        "        print(\"ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    bucket_name = 'de6-team7-bucket'\n",
        "    output_path = f\"s3://{bucket_name}/processed_stock/stock_dt={target_date}/features.parquet\"\n",
        "\n",
        "    print(f\"\\nì „ì²˜ë¦¬ëœ í”¼ì²˜ í…Œì´ë¸”ì„ S3ì— ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "    print(f\"ê²½ë¡œ: {output_path}\")\n",
        "\n",
        "    try:\n",
        "        storage_options = {\n",
        "            'key': userdata.get('AWS_ACCESS_KEY_ID'),\n",
        "            'secret': userdata.get('AWS_SECRET_ACCESS_KEY')\n",
        "        }\n",
        "        df.to_parquet(\n",
        "            output_path,\n",
        "            index=False,\n",
        "            engine='pyarrow',\n",
        "            storage_options=storage_options\n",
        "        )\n",
        "        print(\"âœ… ì €ì¥ ì™„ë£Œ!\")\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ S3 ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    target_date = '2025-07-11'\n",
        "    print(f\"--- ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ({target_date}) ---\")\n",
        "\n",
        "    raw_data = load_data(target_date)\n",
        "\n",
        "    if not raw_data.empty:\n",
        "        feature_df = feature_table(raw_data, target_date)\n",
        "        print(feature_df.head())\n",
        "        save_to_s3(feature_df, target_date)\n",
        "    else:\n",
        "        print(f\"{target_date}ì— ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "    print(f\"--- ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ ({target_date}) ---\")\n"
      ],
      "metadata": {
        "id": "nR28Q9RPeKqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d11ecbf3-e33a-4605-a02f-6388a96cf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ (2025-07-11) ---\n",
            "S3 ê²½ë¡œì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤: s3://de6-team7-bucket/raw_stock/stock_dt=2025-07-11/\n",
            "ì´ 20ê°œì˜ Parquet íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n",
            "ì´ 13,327ê°œì˜ ë¡œìš°(ë°ì´í„°)ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.\n",
            "            timestamp stock_code stock_name  open_price_1h  high_price_1h  \\\n",
            "0 2025-07-11 10:00:00     000020       ë™í™”ì•½í’ˆ         6950.0         6950.0   \n",
            "1 2025-07-11 11:00:00     000020       ë™í™”ì•½í’ˆ         6910.0         6910.0   \n",
            "2 2025-07-11 12:00:00     000020       ë™í™”ì•½í’ˆ         6930.0         6940.0   \n",
            "3 2025-07-11 13:00:00     000020       ë™í™”ì•½í’ˆ         6910.0         6910.0   \n",
            "4 2025-07-11 14:00:00     000020       ë™í™”ì•½í’ˆ         6910.0         6910.0   \n",
            "\n",
            "   low_price_1h  close_price_1h  volume_1h  vwap_1h  spread_mean_1h  \\\n",
            "0        6940.0          6940.0       1836   6890.0            10.0   \n",
            "1        6910.0          6910.0        612   6890.0            20.0   \n",
            "2        6930.0          6940.0       1224   6890.0            15.0   \n",
            "3        6910.0          6910.0        612   6890.0            20.0   \n",
            "4        6910.0          6910.0        612   6890.0            10.0   \n",
            "\n",
            "   orderbook_imbalance_1h  total_book_imbalance_1h   ma_5h  ma_20h  \n",
            "0                0.538861                 0.827538  6940.0  6940.0  \n",
            "1                0.930549                 0.853946  6925.0  6925.0  \n",
            "2                0.438783                 0.868749  6930.0  6930.0  \n",
            "3                0.990295                 0.817703  6925.0  6925.0  \n",
            "4                0.930493                 0.647179  6922.0  6922.0  \n",
            "\n",
            "ì „ì²˜ë¦¬ëœ í”¼ì²˜ í…Œì´ë¸”ì„ S3ì— ì €ì¥í•©ë‹ˆë‹¤...\n",
            "ê²½ë¡œ: s3://de6-team7-bucket/processed_stock/stock_dt=2025-07-11/features.parquet\n",
            "âœ… ì €ì¥ ì™„ë£Œ!\n",
            "--- ì£¼ì‹ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ (2025-07-11) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install s3fs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCSxFV_-w7Ft",
        "outputId": "12289ee2-4f31-47e5-c6d2-9a1438506706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.11/dist-packages (2025.7.0)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.11/dist-packages (from s3fs) (2.23.1)\n",
            "Requirement already satisfied: fsspec==2025.7.0 in /usr/local/lib/python3.11/dist-packages (from s3fs) (2025.7.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from s3fs) (3.11.15)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.38.47,>=1.38.40 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.38.46)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.6.3)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.11/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.14.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.47,>=1.38.40->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snowflake-connector-python[pandas]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbcEuHwkWsOs",
        "outputId": "e3e3f139-424b-46f6-e1c7-ec6111f0b665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting snowflake-connector-python[pandas]\n",
            "  Downloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (71 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.8/71.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python[pandas])\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: boto3>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.39.9)\n",
            "Requirement already satisfied: botocore>=1.24 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.39.9)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2025.7.14)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.14.1)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (4.3.8)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (0.13.3)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<19.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python[pandas]) (18.1.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python[pandas]) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.24->snowflake-connector-python[pandas]) (0.13.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.24->snowflake-connector-python[pandas]) (2.4.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python[pandas]) (2.22)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.1.2->snowflake-connector-python[pandas]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python[pandas]) (1.17.0)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading snowflake_connector_python-3.16.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: asn1crypto, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. 1ì‹œê°„ ë’¤ ê°€ê²© ì˜ˆì¸¡(íšŒê·€)\n"
      ],
      "metadata": {
        "id": "z8BdkMfbsM-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import snowflake.connector\n",
        "from snowflake.connector.pandas_tools import write_pandas\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "\n",
        "def get_snowflake_connection():\n",
        "    return snowflake.connector.connect(\n",
        "        user=userdata.get('SNOWFLAKE_USER'),\n",
        "        password=userdata.get('SNOWFLAKE_PASSWORD'),\n",
        "        account=userdata.get('SNOWFLAKE_ACCOUNT'),\n",
        "        warehouse=userdata.get('SNOWFLAKE_WAREHOUSE'),\n",
        "        database=userdata.get('SNOWFLAKE_DATABASE'),\n",
        "        schema=userdata.get('SNOWFLAKE_SCHEMA')\n",
        "    )\n",
        "\n",
        "# --- ë§ˆì¼“ë³„ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (íšŒê·€) ---\n",
        "def train_stock_regression_model(market_df):\n",
        "    df = market_df.copy()\n",
        "\n",
        "    # Target ë³€ìˆ˜ ìƒì„± ('1ì‹œê°„ ë’¤ ì¢…ê°€')\n",
        "    df['future_1h_price'] = df['close_price_1h'].shift(-1)\n",
        "    df.dropna(subset=['future_1h_price'], inplace=True)\n",
        "    if df.empty:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "    cols_to_drop = df.columns[df.isna().all()].tolist()\n",
        "    feature_cols = df.columns.difference([\n",
        "        'stock_code', 'stock_name', 'timestamp', 'future_1h_price'\n",
        "    ] + cols_to_drop)\n",
        "\n",
        "    X, y = df[feature_cols], df['future_1h_price']\n",
        "\n",
        "    split_idx = int(len(df) * 0.8)\n",
        "    X_train, X_test = X.iloc[:split_idx].copy(), X.iloc[split_idx:].copy()\n",
        "    y_train, y_test = y.iloc[:split_idx].copy(), y.iloc[split_idx:].copy()\n",
        "\n",
        "    model = lgb.LGBMRegressor(objective='regression_l1', random_state=42)\n",
        "    model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)])\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "    result_df = pd.DataFrame({\n",
        "        'timestamp': df.loc[y_test.index, 'timestamp'],\n",
        "        'stock_code': df.loc[y_test.index, 'stock_code'],\n",
        "        'stock_name': df.loc[y_test.index, 'stock_name'],\n",
        "        'actual_price': y_test,\n",
        "        'predicted_price': predictions\n",
        "    })\n",
        "\n",
        "    return mae, result_df\n",
        "\n",
        "def train_and_predict_regression(df):\n",
        "    \"\"\"ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ëª¨ë“  ì¢…ëª©ì˜ íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¢…í•©í•©ë‹ˆë‹¤.\"\"\"\n",
        "    print(\"\\n--- ML ê°€ê²© ì˜ˆì¸¡(íšŒê·€) ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---\")\n",
        "    if df.empty:\n",
        "        print(\"í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    all_stocks = [group.copy() for _, group in df.groupby('stock_code')]\n",
        "    results = []\n",
        "    market_maes = {}\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "        futures = {executor.submit(train_stock_regression_model, stock_df): stock_df['stock_code'].iloc[0] for stock_df in all_stocks}\n",
        "        for future in futures:\n",
        "            stock_code = futures[future]\n",
        "            try:\n",
        "                mae, res_df = future.result()\n",
        "                if res_df is not None:\n",
        "                    results.append(res_df)\n",
        "                    market_maes[stock_code] = mae\n",
        "            except Exception as exc:\n",
        "                print(f'{stock_code} ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}')\n",
        "\n",
        "    if market_maes:\n",
        "        print(\"\\n--- ìƒìœ„ 5ê°œ ëª¨ë¸ ì„±ëŠ¥ (MAEê°€ ë‚®ì€ ìˆœ) ---\")\n",
        "        sorted_maes = sorted(market_maes.items(), key=lambda item: item[1])\n",
        "        for code, mae in sorted_maes[:5]:\n",
        "            print(f\"ì¢…ëª© ì½”ë“œ: {code}, MAE: {mae:.4f}\")\n",
        "        print(\"------------------------------------\")\n",
        "\n",
        "    if not results:\n",
        "        print(\"ì˜ˆì¸¡ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    final_df = pd.concat(results, ignore_index=True)\n",
        "    print(\"âœ… í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ\")\n",
        "    return final_df\n",
        "\n",
        "def upload_to_snowflake(df, table_name, target_date):\n",
        "    if df.empty:\n",
        "        print(\"ì—…ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    staging_table_name = f\"{table_name}_STAGING\"\n",
        "    df['processed_dt'] = pd.to_datetime(target_date).strftime('%Y-%m-%d')\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    conn = None\n",
        "    try:\n",
        "        print(f\"\\n--- Snowflakeì— ê²°ê³¼ ì ì¬ ì‹œì‘ ---\")\n",
        "        conn = get_snowflake_connection()\n",
        "        if conn:\n",
        "            write_pandas(conn, df, staging_table_name.upper(), auto_create_table=True, overwrite=True)\n",
        "            print(f\"âœ… Staging í…Œì´ë¸”({staging_table_name}) ì ì¬ ì„±ê³µ: {df.shape[0]} í–‰\")\n",
        "\n",
        "            cursor = conn.cursor()\n",
        "\n",
        "            delete_query = f\"DELETE FROM {table_name.upper()} WHERE PROCESSED_DT = '{target_date}'\"\n",
        "            cursor.execute(delete_query)\n",
        "            print(f\"âœ… ë³¸ í…Œì´ë¸”({table_name})ì—ì„œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: {cursor.rowcount} í–‰\")\n",
        "\n",
        "            insert_query = f\"INSERT INTO {table_name.upper()} SELECT * FROM {staging_table_name.upper()}\"\n",
        "            cursor.execute(insert_query)\n",
        "            print(\"âœ… ë³¸ í…Œì´ë¸”ë¡œ ë°ì´í„° ì´ë™ ì™„ë£Œ!\")\n",
        "\n",
        "            cursor.close()\n",
        "    except Exception as e:\n",
        "        print(f\"ğŸš¨ Snowflake ì ì¬ ì˜¤ë¥˜: {e}\")\n",
        "    finally:\n",
        "        if conn is not None:\n",
        "            conn.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if 'feature_df' in locals() and not feature_df.empty:\n",
        "        predictions_df = train_and_predict_regression(feature_df)\n",
        "\n",
        "        if not predictions_df.empty:\n",
        "            print(\"\\n--- ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (ìƒìœ„ 5ê°œ) ---\")\n",
        "            print(predictions_df.head())\n",
        "            print(\"--------------------------------\")\n",
        "            upload_to_snowflake(predictions_df, 'STOCK_PRICE_PREDICTION', target_date)\n",
        "    else:\n",
        "        print(\"ì˜¤ë¥˜: 'feature_df'ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì „ ì…€ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCiqBbPXhI8-",
        "outputId": "4b96f1e3-cc76-4812-8fd4-47436aa2753d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ML ê°€ê²© ì˜ˆì¸¡(íšŒê·€) ëª¨ë¸ í•™ìŠµ ì‹œì‘ ---\n",
            "\n",
            "--- ìƒìœ„ 5ê°œ ëª¨ë¸ ì„±ëŠ¥ (MAEê°€ ë‚®ì€ ìˆœ) ---\n",
            "ì¢…ëª© ì½”ë“œ: 000020, MAE: 0.0000\n",
            "ì¢…ëª© ì½”ë“œ: 000300, MAE: 0.0000\n",
            "ì¢…ëª© ì½”ë“œ: 000430, MAE: 0.0000\n",
            "ì¢…ëª© ì½”ë“œ: 000520, MAE: 0.0000\n",
            "ì¢…ëª© ì½”ë“œ: 000650, MAE: 0.0000\n",
            "------------------------------------\n",
            "âœ… í•™ìŠµ ë° ì˜ˆì¸¡ ì™„ë£Œ\n",
            "\n",
            "--- ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ (ìƒìœ„ 5ê°œ) ---\n",
            "            timestamp stock_code stock_name  actual_price  predicted_price\n",
            "0 2025-07-11 13:00:00     000020       ë™í™”ì•½í’ˆ        6910.0           6910.0\n",
            "1 2025-07-11 13:00:00     000040      KRëª¨í„°ìŠ¤         430.0            431.0\n",
            "2 2025-07-11 13:00:00     000050         ê²½ë°©        8150.0           8190.0\n",
            "3 2025-07-11 13:00:00     000070      ì‚¼ì–‘í™€ë”©ìŠ¤      100100.0          99500.0\n",
            "4 2025-07-11 13:00:00     000080      í•˜ì´íŠ¸ì§„ë¡œ       21600.0          21675.0\n",
            "--------------------------------\n",
            "\n",
            "--- Snowflakeì— ê²°ê³¼ ì ì¬ ì‹œì‘ ---\n",
            "âœ… Staging í…Œì´ë¸”(STOCK_PRICE_PREDICTION_STAGING) ì ì¬ ì„±ê³µ: 2770 í–‰\n",
            "âœ… ë³¸ í…Œì´ë¸”(STOCK_PRICE_PREDICTION)ì—ì„œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ: 25693 í–‰\n",
            "âœ… ë³¸ í…Œì´ë¸”ë¡œ ë°ì´í„° ì´ë™ ì™„ë£Œ!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"forkin\"+\"me\")"
      ],
      "metadata": {
        "id": "btivchmWos5j",
        "outputId": "6a3fdc86-bf3c-4a6e-f82f-8aec21982b1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "forkinme\n"
          ]
        }
      ]
    }
  ]
}